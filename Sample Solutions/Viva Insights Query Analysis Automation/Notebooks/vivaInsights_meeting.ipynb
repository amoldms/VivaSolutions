{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 38,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-17T18:43:11.8216303Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-17T18:43:12.0076229Z",
              "execution_finish_time": "2022-01-17T18:43:12.1636874Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 38, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# # #Initializing the parameters\r\n",
        "# # Read about how a Parameter cell should be used for definign and initializing\r\n",
        "# # parameters in Synapse\r\n",
        "\r\n",
        "# Storage Account Name\r\n",
        "StorageAccountName = \"\"\r\n",
        "# Main container/directory on the storage account\r\n",
        "VivaInsightsDataFileSystem = \"\"\r\n",
        "\r\n",
        "PipelineId = \"\"\r\n",
        "MeetingQueryDatasetFolder = \"\"\r\n",
        "SecondaryEmployeeId = \"\"\r\n",
        "\r\n",
        "# Database connection information\r\n",
        "SQLServerEndpoint = \"\"\r\n",
        "DBName = \"\"\r\n",
        "DBUser = \"\"\r\n",
        "DBPass = \"\"\r\n",
        "DBPort = \"\"\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 38,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-17T18:43:12.0263705Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-17T18:43:12.2714043Z",
              "execution_finish_time": "2022-01-17T18:43:12.4247891Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 38, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineId is:  6f714f55-1b3c-4a32-ad47-deb4a3741f3f"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import sys\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import SQLContext\r\n",
        "from pyspark.sql.functions import explode\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "\r\n",
        "# constants_ path template to access storage account for read and write\r\n",
        "inputFilePath = \"abfss://{}@{}.dfs.core.windows.net/{}/raw/{}/*.txt\"\r\n",
        "storageAccount = \"{}.dfs.core.windows.net\"\r\n",
        "outputFilePath = \"https://{}.dfs.core.windows.net/{}/{}\"\r\n",
        "\r\n",
        "#Setting Prameters\r\n",
        "extractionFS = VivaInsightsDataFileSystem\r\n",
        "\r\n",
        "print(\"PipelineId is: \", PipelineId)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 38,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-17T18:43:12.2259454Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-17T18:43:12.5397343Z",
              "execution_finish_time": "2022-01-17T18:43:44.0607469Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 38, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql.functions import split\r\n",
        "from pyspark.sql.types import DateType\r\n",
        "\r\n",
        "#Reading meeting csv file from storage account\r\n",
        "meetingDf = spark.read.csv(inputFilePath.format(extractionFS, StorageAccountName, PipelineId, MeetingQueryDatasetFolder), header = 'true', inferSchema= 'true')\r\n",
        "\r\n",
        "# Cleaning MeetingId column, removing the datetime portion \r\n",
        "meetingDf = meetingDf.withColumn(\"MeetingId\", split(col(\"MeetingId\"), \":\").getItem(0))\r\n",
        "\r\n",
        "\r\n",
        "# Dataframe prep\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumn(\"StartDate\",meetingDf['StartDate'].cast(DateType()))\r\n",
        "meetingDf = meetingDf.withColumn(\"EndDate\",meetingDf['EndDate'].cast(DateType()))\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumn(\"StartTimestampUTC\", to_timestamp(concat_ws(\" \", meetingDf.StartDate, meetingDf.StartTimeUTC)))\r\n",
        "meetingDf = meetingDf.withColumn(\"EndTimestampUTC\", to_timestamp(concat_ws(\" \",meetingDf.EndDate,  meetingDf.EndTimeUTC)))\r\n",
        "\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumnRenamed(\"Organizer_\"+SecondaryEmployeeId,\"Organizer_EmployeeId\")\r\n",
        "\r\n",
        "\r\n",
        "meetingDf.createOrReplaceTempView('meetingDf')\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 38,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-17T18:43:13.5128757Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-17T18:43:45.3183302Z",
              "execution_finish_time": "2022-01-17T18:43:48.0608275Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 38, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest existing meeting date in DB is 2021-11-13 20:58:42"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      },
      "source": [
        "# # Checking the Database for the last existing record\r\n",
        "jdbcHostname = SQLServerEndpoint\r\n",
        "jdbcDatabase = DBName\r\n",
        "jdbcPort = DBPort\r\n",
        "username = DBUser\r\n",
        "password = DBPass\r\n",
        "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n",
        "connectionProperties = {\r\n",
        "   \"user\" : username,\r\n",
        "   \"password\" : password,\r\n",
        "   \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}\r\n",
        "pushdown_query = \"(Select max(StartTimestampUTC) as temp from viva_insights_meeting) tempTbl\"\r\n",
        "latestExistingDate = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties).first().temp\r\n",
        "print(\"Latest existing meeting date in DB is\", latestExistingDate)\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 38,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-17T18:43:14.0948854Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-17T18:43:48.1657096Z",
              "execution_finish_time": "2022-01-17T18:43:49.9802053Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 38, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OutputStatus is:  PartialUpload\nNumber of records inserted is:  9"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Preparing dataframe for upsert/insert into database\r\n",
        "# # Record selection\r\n",
        "if (latestExistingDate == None):\r\n",
        "    outputStatus = \"FullUpload\"\r\n",
        "    outputDf = meetingDf\r\n",
        "else:\r\n",
        "    outputStatus = \"PartialUpload\"\r\n",
        "    latestExistingDate = str(latestExistingDate.date())\r\n",
        "    outputDf = meetingDf[meetingDf.StartTimestampUTC > latestExistingDate]\r\n",
        "\r\n",
        "# Attribute selection\r\n",
        "columns = [\"MeetingId\",\"StartTimestampUTC\",\"EndTimestampUTC\", \"Organizer_PersonId\",\"Organizer_EmployeeId\", \"Organizer_Organization\", \"Organizer_LevelDesignation\"\r\n",
        ", \"Organizer_IsInternal\",\"Attendees\",\"Attendees_with_conflicting_meetings\", \"Invitees\", \"Emails_sent_during_meetings\"\r\n",
        ", \"Instant_messages_sent_during_meetings\" , \"Attendees_multitasking\", \"Attendee_meeting_hours\", \"Redundant_attendees\"\r\n",
        ", \"Total_meeting_cost\", \"Total_redundant_hours\" , \"IsCancelled\", \"DurationHours\", \"IsRecurring\",\"Subject\" ,\"TotalAccept\"\r\n",
        ", \"TotalNoResponse\", \"TotalDecline\" , \"TotalNoEmailsDuringMeeting\", \"TotalNoDoubleBooked\", \"TotalNoAttendees\"\r\n",
        ", \"MeetingResources\", \"BusinessProcesses\"]\r\n",
        "\r\n",
        "outputDf = outputDf.select([col for col in columns])\r\n",
        "\r\n",
        "# display(outputDf)\r\n",
        "print(\"OutputStatus is: \", outputStatus)\r\n",
        "print(\"Number of records inserted is: \", outputDf.count())\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 38,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-17T18:43:14.3694005Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-17T18:43:50.0881364Z",
              "execution_finish_time": "2022-01-17T18:43:53.9353558Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 38, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Insert/Upset into database\r\n",
        "mode = \"append\"\r\n",
        "url = \"jdbc:sqlserver://mgdcvivasynapse.sql.azuresynapse.net:1433;database=VivaInsights\"\r\n",
        "outputDf.write.jdbc(url=jdbcUrl, table=\"dbo.viva_insights_meeting\", mode=mode, properties=connectionProperties)"
      ]
    }
  ]
}