{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # #Initializing the parameters\r\n",
        "# # Read about how a Parameter cell should be used for definign and initializing\r\n",
        "# # parameters in Synapse\r\n",
        "\r\n",
        "# Storage Account Name\r\n",
        "StorageAccountName = \"\"\r\n",
        "# Main container/directory on the storage account\r\n",
        "VivaInsightsDataFileSystem = \"\"\r\n",
        "\r\n",
        "PipelineId = \"\"\r\n",
        "MeetingQueryDatasetFolder = \"\"\r\n",
        "SecondaryEmployeeId = \"\"\r\n",
        "\r\n",
        "# Database connection information\r\n",
        "SQLServerEndpoint = \"\"\r\n",
        "DBName = \"\"\r\n",
        "DBUser = \"\"\r\n",
        "DBPass = \"\"\r\n",
        "DBPort = \"\"\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import SQLContext\r\n",
        "from pyspark.sql.functions import explode\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "\r\n",
        "# constants_ path template to access storage account for read and write\r\n",
        "inputFilePath = \"abfss://{}@{}.dfs.core.windows.net/{}/raw/{}/*.txt\"\r\n",
        "storageAccount = \"{}.dfs.core.windows.net\"\r\n",
        "outputFilePath = \"https://{}.dfs.core.windows.net/{}/{}\"\r\n",
        "\r\n",
        "#Setting Prameters\r\n",
        "extractionFS = VivaInsightsDataFileSystem\r\n",
        "\r\n",
        "print(\"PipelineId is: \", PipelineId)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split\r\n",
        "from pyspark.sql.types import DateType\r\n",
        "\r\n",
        "#Reading meeting csv file from storage account\r\n",
        "meetingDf = spark.read.csv(inputFilePath.format(extractionFS, StorageAccountName, PipelineId, MeetingQueryDatasetFolder), header = 'true', inferSchema= 'true')\r\n",
        "\r\n",
        "# Cleaning MeetingId column, removing the datetime portion \r\n",
        "meetingDf = meetingDf.withColumn(\"MeetingId\", split(col(\"MeetingId\"), \":\").getItem(0))\r\n",
        "\r\n",
        "\r\n",
        "# Dataframe prep\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumn(\"StartDate\",meetingDf['StartDate'].cast(DateType()))\r\n",
        "meetingDf = meetingDf.withColumn(\"EndDate\",meetingDf['EndDate'].cast(DateType()))\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumn(\"StartTimestampUTC\", to_timestamp(concat_ws(\" \", meetingDf.StartDate, meetingDf.StartTimeUTC)))\r\n",
        "meetingDf = meetingDf.withColumn(\"EndTimestampUTC\", to_timestamp(concat_ws(\" \",meetingDf.EndDate,  meetingDf.EndTimeUTC)))\r\n",
        "\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumnRenamed(\"Organizer_\"+SecondaryEmployeeId,\"Organizer_EmployeeId\")\r\n",
        "\r\n",
        "\r\n",
        "meetingDf.createOrReplaceTempView('meetingDf')\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Checking the Database for the last existing record\r\n",
        "jdbcHostname = SQLServerEndpoint\r\n",
        "jdbcDatabase = DBName\r\n",
        "jdbcPort = DBPort\r\n",
        "username = DBUser\r\n",
        "password = DBPass\r\n",
        "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n",
        "connectionProperties = {\r\n",
        "   \"user\" : username,\r\n",
        "   \"password\" : password,\r\n",
        "   \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}\r\n",
        "pushdown_query = \"(Select max(StartTimestampUTC) as temp from viva_insights_meeting) tempTbl\"\r\n",
        "latestExistingDate = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties).first().temp\r\n",
        "print(\"Latest existing meeting date in DB is\", latestExistingDate)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataframe for upsert/insert into database\r\n",
        "# # Record selection\r\n",
        "if (latestExistingDate == None):\r\n",
        "    outputStatus = \"FullUpload\"\r\n",
        "    outputDf = meetingDf\r\n",
        "else:\r\n",
        "    outputStatus = \"PartialUpload\"\r\n",
        "    latestExistingDate = str(latestExistingDate.date())\r\n",
        "    outputDf = meetingDf[meetingDf.StartTimestampUTC > latestExistingDate]\r\n",
        "\r\n",
        "# Attribute selection\r\n",
        "columns = [\"MeetingId\",\"StartTimestampUTC\",\"EndTimestampUTC\", \"Organizer_PersonId\",\"Organizer_EmployeeId\", \"Organizer_Organization\", \"Organizer_LevelDesignation\"\r\n",
        ", \"Organizer_IsInternal\",\"Attendees\",\"Attendees_with_conflicting_meetings\", \"Invitees\", \"Emails_sent_during_meetings\"\r\n",
        ", \"Instant_messages_sent_during_meetings\" , \"Attendees_multitasking\", \"Attendee_meeting_hours\", \"Redundant_attendees\"\r\n",
        ", \"Total_meeting_cost\", \"Total_redundant_hours\" , \"IsCancelled\", \"DurationHours\", \"IsRecurring\",\"Subject\" ,\"TotalAccept\"\r\n",
        ", \"TotalNoResponse\", \"TotalDecline\" , \"TotalNoEmailsDuringMeeting\", \"TotalNoDoubleBooked\", \"TotalNoAttendees\"\r\n",
        ", \"MeetingResources\", \"BusinessProcesses\"]\r\n",
        "\r\n",
        "outputDf = outputDf.select([col for col in columns])\r\n",
        "\r\n",
        "# display(outputDf)\r\n",
        "print(\"OutputStatus is: \", outputStatus)\r\n",
        "print(\"Number of records inserted is: \", outputDf.count())\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert/Upset into database\r\n",
        "mode = \"append\"\r\n",
        "url = \"jdbc:sqlserver://mgdcvivasynapse.sql.azuresynapse.net:1433;database=VivaInsights\"\r\n",
        "outputDf.write.jdbc(url=jdbcUrl, table=\"dbo.viva_insights_meeting\", mode=mode, properties=connectionProperties)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}