{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # #Initializing the parameters\r\n",
        "# # Read about how a Parameter cell should be used for definign and initializing\r\n",
        "# # parameters in Synapse\r\n",
        "\r\n",
        "# Storage Account Name\r\n",
        "StorageAccountName = \"\"\r\n",
        "# Main container/directory on the storage account\r\n",
        "VivaInsightsDataFileSystem = \"\"\r\n",
        "\r\n",
        "PipelineId = \"\"\r\n",
        "PersonQueryDatasetFolder = \"\"\r\n",
        "SecondaryEmployeeId = \"\"\r\n",
        "\r\n",
        "# Database connection information\r\n",
        "SQLServerEndpoint = \"\"\r\n",
        "DBName = \"\"\r\n",
        "DBUser = \"\"\r\n",
        "DBPass = \"\"\r\n",
        "DBPort = \"\"\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\r\n",
        "import json\r\n",
        "\r\n",
        "\r\n",
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import SQLContext\r\n",
        "from pyspark.sql.functions import explode\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "\r\n",
        "# constants_ path template to access storage account for read and write\r\n",
        "inputFilePath = \"abfss://{}@{}.dfs.core.windows.net/{}/raw/{}/*.txt\"\r\n",
        "\r\n",
        "storageAccount = \"{}.dfs.core.windows.net\"\r\n",
        "outputFilePath = \"https://{}.dfs.core.windows.net/{}/{}\"\r\n",
        "\r\n",
        "\r\n",
        "#Setting Prameters\r\n",
        "extractionFS = VivaInsightsDataFileSystem\r\n",
        "\r\n",
        "\r\n",
        "print(\"PipelineId is: \", PipelineId)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\r\n",
        "\r\n",
        "#Reading meeting csv file from storage account\r\n",
        "personDf = spark.read.csv(inputFilePath.format(extractionFS, StorageAccountName, PipelineId, PersonQueryDatasetFolder), header = 'true', inferSchema= 'true')\r\n",
        "\r\n",
        "# Dataframe prep\r\n",
        "personDf = personDf.withColumn(\"Date\",personDf['Date'].cast(DateType()))\r\n",
        "personDf = personDf.withColumnRenamed(SecondaryEmployeeId,\"EmployeeId\")\r\n",
        "\r\n",
        "# personDf.printSchema()\r\n",
        "personDf.createOrReplaceTempView('personDf')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Checking the Database for the last existing record\r\n",
        "jdbcHostname = SQLServerEndpoint\r\n",
        "jdbcDatabase = DBName\r\n",
        "jdbcPort = DBPort\r\n",
        "username = DBUser\r\n",
        "password = DBPass\r\n",
        "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n",
        "connectionProperties = {\r\n",
        "   \"user\" : username,\r\n",
        "   \"password\" : password,\r\n",
        "   \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}\r\n",
        "pushdown_query = \"(Select max(Date) as temp from viva_insights_person) tempTbl\"\r\n",
        "latestExistingDate = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties).first().temp\r\n",
        "print(\"Latest existing person date in DB is\", latestExistingDate)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataframe for upsert/insert into database\r\n",
        "# # Record selection\r\n",
        "if (latestExistingDate == None):\r\n",
        "    outputStatus = \"FullUpload\"\r\n",
        "    outputDf = personDf\r\n",
        "else:\r\n",
        "    outputStatus = \"PartialUpload\"\r\n",
        "    outputDf = personDf[personDf.Date > latestExistingDate]\r\n",
        "\r\n",
        "\r\n",
        "# Attribute selection\r\n",
        "\r\n",
        "columns = [\"PersonId\",\"EmployeeId\", \"Date\", \"Organization\", \"LevelDesignation\", \"Workweek_span\", \"Meetings_with_skip_level\", \"Meeting_hours_with_skip_level\"\r\n",
        ", \"Generated_workload_email_hours\", \"Generated_workload_email_recipients\", \"Generated_workload_instant_messages_hours\"\r\n",
        ", \"Generated_workload_instant_messages_recipients\", \"Generated_reactions_to_posts\", \"Generated_replies_to_posts\"\r\n",
        ", \"Generated_workload_call_hours\", \"Generated_workload_call_participants\", \"Generated_workload_calls_organized\"\r\n",
        ", \"External_network_size\", \"Internal_network_size\", \"Networking_outside_company\", \"Networking_outside_organization\"\r\n",
        ", \"Multitasking_hours\", \"After_hours_meeting_hours\", \"Open_1_hour_block\", \"Open_2_hour_blocks\", \"Total_focus_hours\"\r\n",
        ", \"Low_quality_meeting_hours\", \"Meetings\", \"Meeting_hours\", \"Conflicting_meeting_hours\", \"Multitasking_meeting_hours\"\r\n",
        ", \"Redundant_meeting_hours__lower_level_\", \"Redundant_meeting_hours__organizational_\"\r\n",
        ", \"Time_in_self_organized_meetings\", \"Meeting_hours_during_working_hours\", \"Generated_workload_meeting_attendees\"\r\n",
        ", \"Generated_workload_meeting_hours\", \"Generated_workload_meetings_organized\", \"Manager_coaching_hours_1_on_1\"\r\n",
        ", \"Meetings_with_manager\", \"Meeting_hours_with_manager\", \"Meetings_with_manager_1_on_1\"\r\n",
        ", \"Meeting_hours_with_manager_1_on_1\", \"After_hours_instant_messages\", \"Instant_messages_sent\", \"Instant_Message_hours\"\r\n",
        ", \"Working_hours_instant_messages\", \"Emails_sent\", \"Email_hours\", \"Uninterrupted_focus_hours\"\r\n",
        ", \"After_hours_collaboration_hours\", \"Collaboration_hours_external\", \"Collaboration_hours\"\r\n",
        ", \"Working_hours_collaboration_hours\", \"After_hours_email_hours\", \"Working_hours_email_hours\"\r\n",
        ", \"Channels_with_active_engagement\", \"Teams_with_active_engagement\", \"After_hours_channel_message_hours\"\r\n",
        ", \"Channel_message_hours\", \"Channel_messages_sent\", \"Channel_reactions\", \"Channel_visits\"\r\n",
        ", \"Working_hours_channel_message_hours\", \"After_hours_in_calls\", \"Total_calls\", \"Call_hours\"\r\n",
        ", \"Working_hours_in_calls\", \"IsInternal\", \"IsActive\", \"WorkingStartTimeSetInOutlook\", \"WorkingEndTimeSetInOutlook\"\r\n",
        ", \"WorkingDaysSetInOutlook\"]\r\n",
        "\r\n",
        "outputDf = outputDf.select([col for col in columns])\r\n",
        "\r\n",
        "# display(outputDf)\r\n",
        "print(\"OutputStatus is: \", outputStatus)\r\n",
        "print(\"Number of records inserted is: \", outputDf.count())\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert/Upset into database\r\n",
        "\r\n",
        "mode = \"append\"\r\n",
        "url = \"jdbc:sqlserver://mgdcvivasynapse.sql.azuresynapse.net:1433;database=VivaInsights\"\r\n",
        "outputDf.write.jdbc(url=jdbcUrl, table=\"dbo.viva_insights_person\", mode=mode, properties=connectionProperties)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}