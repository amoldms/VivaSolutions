{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # #Initializing the parameters\r\n",
        "# # Read about how a Parameter cell should be used for definign and initializing\r\n",
        "# # parameters in Synapse\r\n",
        "\r\n",
        "# Storage Account Name\r\n",
        "StorageAccountName = \"\"\r\n",
        "# Main container/directory on the storage account\r\n",
        "VivaInsightsDataFileSystem = \"\"\r\n",
        "\r\n",
        "PipelineId = \"\"\r\n",
        "MeetingQueryDatasetFolder = \"\"\r\n",
        "SecondaryEmployeeId = \"\"\r\n",
        "\r\n",
        "# Database connection information\r\n",
        "SQLServerEndpoint = \"\"\r\n",
        "DBName = \"\"\r\n",
        "DBUser = \"\"\r\n",
        "DBPass = \"\"\r\n",
        "DBPort = \"\"\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 42,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:46:45.8273339Z",
              "session_start_time": "2022-01-20T06:46:45.8684334Z",
              "execution_start_time": "2022-01-20T06:47:38.7306908Z",
              "execution_finish_time": "2022-01-20T06:47:38.8748643Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 42, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DateType\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "\r\n",
        "print(\"PipelineId is: \", PipelineId)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 42,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:46:45.8328152Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:47:38.971886Z",
              "execution_finish_time": "2022-01-20T06:47:39.1308196Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 42, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineId is:  6f714f55-1b3c-4a32-ad47-deb4a3741f3f"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "#Reading meeting csv file from storage account\r\n",
        "inputFilePath = 'abfss://{0}@{1}.dfs.core.windows.net/{2}/raw/{3}/*.txt'.format(VivaInsightsDataFileSystem, StorageAccountName, PipelineId, MeetingQueryDatasetFolder)\r\n",
        "meetingDf = spark.read.csv(inputFilePath, header = 'true', inferSchema= 'true')\r\n",
        "\r\n",
        "# Cleaning MeetingId column, removing the datetime portion \r\n",
        "meetingDf = meetingDf.withColumn(\"MeetingId\", split(col(\"MeetingId\"), \":\").getItem(0))\r\n",
        "\r\n",
        "\r\n",
        "# Dataframe prep\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumn(\"StartDate\",meetingDf['StartDate'].cast(DateType()))\r\n",
        "meetingDf = meetingDf.withColumn(\"EndDate\",meetingDf['EndDate'].cast(DateType()))\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumn(\"StartTimestampUTC\", to_timestamp(concat_ws(\" \", meetingDf.StartDate, meetingDf.StartTimeUTC)))\r\n",
        "meetingDf = meetingDf.withColumn(\"EndTimestampUTC\", to_timestamp(concat_ws(\" \",meetingDf.EndDate,  meetingDf.EndTimeUTC)))\r\n",
        "\r\n",
        "\r\n",
        "meetingDf = meetingDf.withColumnRenamed(\"Organizer_\"+SecondaryEmployeeId,\"Organizer_EmployeeId\")\r\n",
        "\r\n",
        "\r\n",
        "meetingDf.createOrReplaceTempView('meetingDf')\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 42,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:46:45.8388105Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:47:39.234194Z",
              "execution_finish_time": "2022-01-20T06:48:06.1989856Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 42, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Checking the Database for the last existing record\r\n",
        "jdbcHostname = SQLServerEndpoint\r\n",
        "jdbcDatabase = DBName\r\n",
        "jdbcPort = DBPort\r\n",
        "\r\n",
        "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n",
        "connectionProperties = {\r\n",
        "   \"user\" : DBUser,\r\n",
        "   \"password\" : DBPass,\r\n",
        "   \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}\r\n",
        "pushdown_query = \"(Select max(StartTimestampUTC) as temp from viva_insights_meeting) tempTbl\"\r\n",
        "latestExistingDate = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties).first().temp\r\n",
        "print(\"Latest existing meeting date in DB is\", latestExistingDate)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 42,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:46:45.8406833Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:48:06.3223779Z",
              "execution_finish_time": "2022-01-20T06:48:08.1141761Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 42, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest existing meeting date in DB is 2021-11-13 20:58:42"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataframe for upsert/insert into database\r\n",
        "# # Record selection\r\n",
        "if (latestExistingDate == None):\r\n",
        "    outputStatus = \"FullUpload\"\r\n",
        "    outputDf = meetingDf\r\n",
        "else:\r\n",
        "    outputStatus = \"PartialUpload\"\r\n",
        "    latestExistingDate = str(latestExistingDate.date())\r\n",
        "    outputDf = meetingDf[meetingDf.StartTimestampUTC > latestExistingDate]\r\n",
        "\r\n",
        "# Attribute selection\r\n",
        "columns = [\"MeetingId\",\"StartTimestampUTC\",\"EndTimestampUTC\", \"Organizer_PersonId\",\"Organizer_EmployeeId\", \"Organizer_Organization\", \"Organizer_LevelDesignation\"\r\n",
        ", \"Organizer_IsInternal\",\"Attendees\",\"Attendees_with_conflicting_meetings\", \"Invitees\", \"Emails_sent_during_meetings\"\r\n",
        ", \"Instant_messages_sent_during_meetings\" , \"Attendees_multitasking\", \"Attendee_meeting_hours\", \"Redundant_attendees\"\r\n",
        ", \"Total_meeting_cost\", \"Total_redundant_hours\" , \"IsCancelled\", \"DurationHours\", \"IsRecurring\",\"Subject\" ,\"TotalAccept\"\r\n",
        ", \"TotalNoResponse\", \"TotalDecline\" , \"TotalNoEmailsDuringMeeting\", \"TotalNoDoubleBooked\", \"TotalNoAttendees\"\r\n",
        ", \"MeetingResources\", \"BusinessProcesses\"]\r\n",
        "\r\n",
        "outputDf = outputDf.select([col for col in columns])\r\n",
        "\r\n",
        "# display(outputDf)\r\n",
        "print(\"OutputStatus is: \", outputStatus)\r\n",
        "print(\"Number of records inserted is: \", outputDf.count())\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 42,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:46:45.8466613Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:48:08.2170783Z",
              "execution_finish_time": "2022-01-20T06:48:10.0230208Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 42, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OutputStatus is:  PartialUpload\nNumber of records inserted is:  9"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert/Upsert into database\r\n",
        "mode = \"append\"\r\n",
        "outputDf.write.jdbc(url=jdbcUrl, table=\"dbo.viva_insights_meeting\", mode=mode, properties=connectionProperties)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 42,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:46:45.8489504Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:48:10.1461479Z",
              "execution_finish_time": "2022-01-20T06:48:15.4118175Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 42, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}