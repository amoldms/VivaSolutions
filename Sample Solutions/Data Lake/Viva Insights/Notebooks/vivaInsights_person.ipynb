{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # #Initializing the parameters\r\n",
        "# # Read about how a Parameter cell should be used for defining and initializing parameters in Synapse\r\n",
        "\r\n",
        "# Storage Account Name\r\n",
        "StorageAccountName = \"\"\r\n",
        "# Main container/directory on the storage account\r\n",
        "VivaInsightsDataFileSystem = \"\"\r\n",
        "\r\n",
        "PipelineId = \"\"\r\n",
        "PersonQueryDatasetFolder = \"\"\r\n",
        "SecondaryEmployeeId = \"\"\r\n",
        "\r\n",
        "# Database connection information\r\n",
        "SQLServerEndpoint = \"\"\r\n",
        "DBName = \"\"\r\n",
        "DBUser = \"\"\r\n",
        "DBPass = \"\"\r\n",
        "DBPort = \"\"\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 41,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:35:38.4341377Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:35:38.5661203Z",
              "execution_finish_time": "2022-01-20T06:35:38.7142402Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 41, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import DateType\r\n",
        "\r\n",
        "\r\n",
        "# constants_ path template to access storage account for read and write\r\n",
        "# inputFilePath = \"abfss://{}@{}.dfs.core.windows.net/{}/raw/{}/*.txt\"\r\n",
        "\r\n",
        "# storageAccount = \"{}.dfs.core.windows.net\"\r\n",
        "# # outputFilePath = \"https://{}.dfs.core.windows.net/{}/{}\"\r\n",
        "# outputFilePath = \"https://%s.dfs.core.windows.net/%s/%s\"\r\n",
        "\r\n",
        "#Setting Prameters\r\n",
        "# extractionFS = VivaInsightsDataFileSystem\r\n",
        "\r\n",
        "\r\n",
        "print(\"PipelineId is: \", PipelineId)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 41,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:35:38.5482012Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:35:38.8152464Z",
              "execution_finish_time": "2022-01-20T06:35:38.9736374Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 41, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineId is:  6f714f55-1b3c-4a32-ad47-deb4a3741f3f"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading meeting csv file from storage account\r\n",
        "inputFilePath = 'abfss://{0}@{1}.dfs.core.windows.net/{2}/raw/{3}/*.txt'.format(VivaInsightsDataFileSystem, StorageAccountName, PipelineId, PersonQueryDatasetFolder)\r\n",
        "personDf = spark.read.csv(inputFilePath, header = 'true', inferSchema= 'true')\r\n",
        "\r\n",
        "# Dataframe prep\r\n",
        "personDf = personDf.withColumn(\"Date\",personDf['Date'].cast(DateType()))\r\n",
        "personDf = personDf.withColumnRenamed(SecondaryEmployeeId,\"EmployeeId\")\r\n",
        "\r\n",
        "# personDf.printSchema()\r\n",
        "personDf.createOrReplaceTempView('personDf')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 41,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:35:38.6618099Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:35:39.0974695Z",
              "execution_finish_time": "2022-01-20T06:35:40.171125Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 41, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Checking the Database for the last existing record\r\n",
        "jdbcHostname = SQLServerEndpoint\r\n",
        "jdbcDatabase = DBName\r\n",
        "jdbcPort = DBPort\r\n",
        "\r\n",
        "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\r\n",
        "connectionProperties = {\r\n",
        "   \"user\" : DBUser,\r\n",
        "   \"password\" : DBPass,\r\n",
        "   \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n",
        "}\r\n",
        "pushdown_query = \"(Select max(Date) as temp from viva_insights_person) tempTbl\"\r\n",
        "latestExistingDate = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties).first().temp\r\n",
        "print(\"Latest existing person date in DB is\", latestExistingDate)\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 41,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:35:38.942825Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:35:40.2823033Z",
              "execution_finish_time": "2022-01-20T06:35:41.3557888Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 41, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest existing person date in DB is 2021-11-07"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing dataframe for upsert/insert into database\r\n",
        "# # Record selection\r\n",
        "if (latestExistingDate == None):\r\n",
        "    outputStatus = \"FullUpload\"\r\n",
        "    outputDf = personDf\r\n",
        "else:\r\n",
        "    outputStatus = \"PartialUpload\"\r\n",
        "    outputDf = personDf[personDf.Date > latestExistingDate]\r\n",
        "\r\n",
        "\r\n",
        "# Attribute selection\r\n",
        "\r\n",
        "columns = [\"PersonId\",\"EmployeeId\", \"Date\", \"Organization\", \"LevelDesignation\", \"Workweek_span\", \"Meetings_with_skip_level\", \"Meeting_hours_with_skip_level\"\r\n",
        ", \"Generated_workload_email_hours\", \"Generated_workload_email_recipients\", \"Generated_workload_instant_messages_hours\"\r\n",
        ", \"Generated_workload_instant_messages_recipients\", \"Generated_reactions_to_posts\", \"Generated_replies_to_posts\"\r\n",
        ", \"Generated_workload_call_hours\", \"Generated_workload_call_participants\", \"Generated_workload_calls_organized\"\r\n",
        ", \"External_network_size\", \"Internal_network_size\", \"Networking_outside_company\", \"Networking_outside_organization\"\r\n",
        ", \"Multitasking_hours\", \"After_hours_meeting_hours\", \"Open_1_hour_block\", \"Open_2_hour_blocks\", \"Total_focus_hours\"\r\n",
        ", \"Low_quality_meeting_hours\", \"Meetings\", \"Meeting_hours\", \"Conflicting_meeting_hours\", \"Multitasking_meeting_hours\"\r\n",
        ", \"Redundant_meeting_hours__lower_level_\", \"Redundant_meeting_hours__organizational_\"\r\n",
        ", \"Time_in_self_organized_meetings\", \"Meeting_hours_during_working_hours\", \"Generated_workload_meeting_attendees\"\r\n",
        ", \"Generated_workload_meeting_hours\", \"Generated_workload_meetings_organized\", \"Manager_coaching_hours_1_on_1\"\r\n",
        ", \"Meetings_with_manager\", \"Meeting_hours_with_manager\", \"Meetings_with_manager_1_on_1\"\r\n",
        ", \"Meeting_hours_with_manager_1_on_1\", \"After_hours_instant_messages\", \"Instant_messages_sent\", \"Instant_Message_hours\"\r\n",
        ", \"Working_hours_instant_messages\", \"Emails_sent\", \"Email_hours\", \"Uninterrupted_focus_hours\"\r\n",
        ", \"After_hours_collaboration_hours\", \"Collaboration_hours_external\", \"Collaboration_hours\"\r\n",
        ", \"Working_hours_collaboration_hours\", \"After_hours_email_hours\", \"Working_hours_email_hours\"\r\n",
        ", \"Channels_with_active_engagement\", \"Teams_with_active_engagement\", \"After_hours_channel_message_hours\"\r\n",
        ", \"Channel_message_hours\", \"Channel_messages_sent\", \"Channel_reactions\", \"Channel_visits\"\r\n",
        ", \"Working_hours_channel_message_hours\", \"After_hours_in_calls\", \"Total_calls\", \"Call_hours\"\r\n",
        ", \"Working_hours_in_calls\", \"IsInternal\", \"IsActive\", \"WorkingStartTimeSetInOutlook\", \"WorkingEndTimeSetInOutlook\"\r\n",
        ", \"WorkingDaysSetInOutlook\"]\r\n",
        "\r\n",
        "outputDf = outputDf.select([col for col in columns])\r\n",
        "\r\n",
        "# display(outputDf)\r\n",
        "print(\"OutputStatus is: \", outputStatus)\r\n",
        "print(\"Number of records inserted is: \", outputDf.count())\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 41,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:35:39.3608193Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:35:41.4941284Z",
              "execution_finish_time": "2022-01-20T06:35:42.5569965Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 41, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OutputStatus is:  PartialUpload\nNumber of records inserted is:  0"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert/Upsert into database\r\n",
        "\r\n",
        "mode = \"append\"\r\n",
        "outputDf.write.jdbc(url=jdbcUrl, table=\"dbo.viva_insights_person\", mode=mode, properties=connectionProperties)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool2",
              "session_id": 41,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-01-20T06:35:39.778591Z",
              "session_start_time": null,
              "execution_start_time": "2022-01-20T06:35:42.6897131Z",
              "execution_finish_time": "2022-01-20T06:35:47.9450205Z"
            },
            "text/plain": "StatementMeta(SparkPool2, 41, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}