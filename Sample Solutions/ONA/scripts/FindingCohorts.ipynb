{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Cohort Analysis**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing libraries\r\n",
        "from pyspark.sql.functions import rank, dense_rank, desc,col, when, max, countDistinct, udf\r\n",
        "from pyspark.sql import Window\r\n",
        "from pyspark.sql.types import StringType\r\n",
        "from datetime import datetime\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.datasets import load_iris #, load_breast_cancer\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.tree import _tree"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all the HR attributes that you want to use in creating cohorts\n",
        "# Make sure to add isInfluencer as well\n",
        "all_attr = [\"Organization\",\"FunctionType\",\"Layer\",\"LevelDesignation\",\"Region\",\"SupervisorIndicator\",\"Influence_rank\"]\n",
        "#List  categorical attributes from the attributes selected above\n",
        "categorical_attributes = [\"Organization\",\"FunctionType\",\"Layer\",\"LevelDesignation\",\"Region\",\"SupervisorIndicator\"]\n",
        "#List  none categorical attributes from the attributes selected above\n",
        "non_categorical_attributes = []"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1:** read the input file\r\n",
        "#Note: Modify this cell to read the input data: csv from a local path, Azure blob storage with access key, etc"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This method is reading the input file from Synapse linked Storage account\r\n",
        "inputFilePath = \"abfss://{}@{}.dfs.core.windows.net/{}/*\".format(\"cohortanalysis\",\"mgdcvivadatalake\",\"influenceQuery\")\r\n",
        "df = spark.read.format(\"csv\").option(\"header\",\"True\").load(inputFilePath)\r\n",
        "# display(df)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Config and settings\n",
        "Directed = True\n",
        "Reversed = False\n",
        "InteractionType = \"all\"\n",
        "depth = 4\n",
        "min_sample_leaf_size = 30\n",
        "percentSelection = 0.3"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2:** Add isInfluencer flag"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selection = df.count()*percentSelection\r\n",
        "df = df.withColumn(\"isInfluencer\", when(col(\"Influence_rank\")<=selection, 1).otherwise(0))\r\n",
        "\r\n",
        "numOfChampions = df.filter(col(\"isInfluencer\")==1).select(\"PersonId\").distinct().count()\r\n",
        "populationSize = df.select(\"PersonId\").distinct().count()\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Casting the dataframe to Pandas dataframe in order to use python code\n",
        "pd_df= df.toPandas()\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: ** Training a DecisionTree model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a one hot encoding out of the categorical features that we want to include in cohort creation\n",
        "\n",
        "one_hot_data = pd.get_dummies(pd_df[categorical_attributes], drop_first=False, prefix_sep='=')\n",
        "\n",
        "\n",
        "\n",
        "all_data = pd.concat([pd_df[non_categorical_attributes], one_hot_data], axis=1)\n",
        "\n",
        "\n",
        "# Training DT model\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42, max_depth=depth, criterion='entropy', min_samples_leaf=min_sample_leaf_size)\n",
        "\n",
        "model.fit(all_data, pd_df[[\"isInfluencer\"]])\n",
        "\n",
        "cohorts = pd.DataFrame()\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4:**\r\n",
        "\r\n",
        "## DecisionTree text representation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\r\n",
        "text_representation = tree.export_text(model)\r\n",
        "print(text_representation)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_names"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Printing Cohorts and the tree"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract rules\r\n",
        "\r\n",
        "def tree_to_code(tree, feature_names):\r\n",
        "\r\n",
        "    tree_ = tree.tree_\r\n",
        "\r\n",
        "    feature_name = [\r\n",
        "\r\n",
        "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\r\n",
        "\r\n",
        "        for i in tree_.feature\r\n",
        "\r\n",
        "    ]\r\n",
        "\r\n",
        "#     print( \"def tree({}):\".format(\", \".join(feature_names)))\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "    def recurse(node, depth, stack, l):\r\n",
        "\r\n",
        "        indent = \"  \" * depth\r\n",
        "\r\n",
        "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\r\n",
        "\r\n",
        "            name = feature_name[node]\r\n",
        "            name_1 = feature_name[node].split(\"=\")[0]\r\n",
        "            name_2 = feature_name[node].split(\"=\")[1]\r\n",
        "\r\n",
        "            threshold = tree_.threshold[node]\r\n",
        "            stack.append(name_1 + \"=not_\" + name_2)\r\n",
        "\r\n",
        "            recurse(tree_.children_left[node], depth + 1,stack, l)\r\n",
        "\r\n",
        "            stack.append(name)\r\n",
        "\r\n",
        "            recurse(tree_.children_right[node], depth + 1, stack, l)\r\n",
        "\r\n",
        "        else:\r\n",
        "            d={}\r\n",
        "            for feature in stack:\r\n",
        "              if feature.split(\"=\")[0] in d:\r\n",
        "                if d[feature.split(\"=\")[0]][4:]==str(feature.split(\"=\")[1]):\r\n",
        "                  d[feature.split(\"=\")[0]]= str(feature.split(\"=\")[1])\r\n",
        "                else:\r\n",
        "                  d[feature.split(\"=\")[0]]= str(d[feature.split(\"=\")[0]])+\"_AND_\"+str(feature.split(\"=\")[1])\r\n",
        "              else:\r\n",
        "                d[feature.split(\"=\")[0]]= str(feature.split(\"=\")[1])\r\n",
        "              d[\"Number of Influencers\"] = tree_.value[node][0][1]\r\n",
        "              d[\"Number of non-Influencers\"]=tree_.value[node][0][0]\r\n",
        "            l.append(pd.Series(d))\r\n",
        "            stack.pop()\r\n",
        "            # print( \"{}return {} Percentage of champions in group: {}, Percentage of all champions: {}\".format(indent, tree_.value[node],round((tree_.value[node][0][1])/(tree_.value[node][0][0]+tree_.value[node][0][1]),2),round(tree_.value[node][0][1]/numOfChampions,2)))\r\n",
        "\r\n",
        " \r\n",
        "    l = []\r\n",
        "    recurse(0, 1,[],l) \r\n",
        "    return pd.DataFrame(l)\r\n",
        "\r\n",
        "def tree_to_pseudo(tree, feature_names):\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "              '''\r\n",
        "\r\n",
        "              Outputs a decision tree model as if/then pseudocode\r\n",
        "\r\n",
        "             \r\n",
        "\r\n",
        "              Parameters:\r\n",
        "\r\n",
        "              -----------\r\n",
        "\r\n",
        "              tree: decision tree model\r\n",
        "\r\n",
        "                           The decision tree to represent as pseudocode\r\n",
        "\r\n",
        "              feature_names: list\r\n",
        "\r\n",
        "                           The feature names of the dataset used for building the decision tree\r\n",
        "\r\n",
        "              '''\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "              left = tree.tree_.children_left\r\n",
        "\r\n",
        "              right = tree.tree_.children_right\r\n",
        "\r\n",
        "              threshold = tree.tree_.threshold\r\n",
        "\r\n",
        "              features = [feature_names[i] for i in tree.tree_.feature]\r\n",
        "\r\n",
        "              value = tree.tree_.value\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Different output format\r\n",
        "\r\n",
        "# python inden\r\n",
        "\r\n",
        "col_names = [str(x) for x in list(all_data.columns.values)]\r\n",
        "\r\n",
        "all_data.columns = col_names\r\n",
        "\r\n",
        "cohorts = tree_to_code(model, col_names).fillna(\"\")\r\n",
        "cohorts[\"cohort size\"] = round(cohorts[\"Number of Influencers\"]+cohorts[\"Number of non-Influencers\"],3)\r\n",
        "cohorts[\"% covered Influencers\"] = round(cohorts[\"Number of Influencers\"]/numOfChampions,3)\r\n",
        "cohorts[\"% Influencers in cohorts\"] = round(cohorts[\"Number of Influencers\"]/cohorts[\"cohort size\"],3)\r\n",
        "cohorts[\"% population\"] = round(cohorts[\"cohort size\"]/populationSize,3)\r\n",
        "\r\n",
        "tree_to_pseudo(model, col_names)\r\n",
        "\r\n",
        "display(cohorts)\r\n",
        "\r\n",
        "# from sklearn import tree\r\n",
        "\r\n",
        "# # dotfile = open(\"dt.dot\", 'w')\r\n",
        "\r\n",
        "# # tree.export_graphviz(model, out_file=dotfile, feature_names=raw_data.feature_names)\r\n",
        "\r\n",
        "# dotfile.close() "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "fig = plt.figure(figsize=(25,20))\r\n",
        "_ = tree.plot_tree(model, \r\n",
        "                   feature_names=col_names,  \r\n",
        "                   class_names=\"isInfluencer\",\r\n",
        "                   filled=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(25,20))\r\n",
        "_ = tree.plot_tree(model, feature_names=col_names, filled=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}