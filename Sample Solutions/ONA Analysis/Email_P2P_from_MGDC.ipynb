{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook creates a person-to-person matrix from MGDC _email_ data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "StartDate = \"2018-09-01\"\r\n",
        "EndDate = \"2019-03-31\"\r\n",
        "Destination = \"ona/test_20210730\"\r\n",
        "hr_attributes = \"FunctionType,LevelDesignation,Domain,Organization,Region\"\r\n",
        "minGroupSize = 5\r\n",
        "log_folder = \"ona/job_logs/01/\"\r\n",
        "meaningfulParticipantThreshold = 8\r\n",
        "metric_partition, metric_clustering, metric_fluidity, metric_xy, metric_ari, metric_freedom = True, True, True, True, True, True\r\n",
        "bad_flag = False # becomes True if LCC = 0, LCC fails, data is less than 2 months, or modularity fails.\r\n",
        "\r\n",
        "ResultBlobPath = 'abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_data/'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "session_error",
              "livy_statement_state": null,
              "queued_time": "2021-08-12T21:55:47.1673514Z",
              "session_start_time": "2021-08-12T21:55:47.2071472Z",
              "execution_start_time": null,
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(, , , SessionError, )"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AVAILABLE_COMPUTE_CAPACITY_EXCEEDED",
          "evalue": "Livy session has failed. Error code: AVAILABLE_COMPUTE_CAPACITY_EXCEEDED. Your job requested 12 vcores. However, the pool only has 0 vcores available out of quota of 12 vcores. Try ending the running job(s) in the pool, reducing the numbers of vcores requested, increasing the pool maximum size or using another pool. Source: User.",
          "traceback": [
            "AVAILABLE_COMPUTE_CAPACITY_EXCEEDED: Livy session has failed. Error code: AVAILABLE_COMPUTE_CAPACITY_EXCEEDED. Your job requested 12 vcores. However, the pool only has 0 vcores available out of quota of 12 vcores. Try ending the running job(s) in the pool, reducing the numbers of vcores requested, increasing the pool maximum size or using another pool. Source: User."
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, date_format, ceil, year, lit, udf, explode, split, last_day, trunc, monotonically_increasing_id\r\n",
        "import os\r\n",
        "import pyspark.sql.functions as F\r\n",
        "import datetime as dt\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import col\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "import networkx as nx\r\n",
        "import topologic as tc\r\n",
        "import graspologic as gc\r\n",
        "\r\n",
        "from datetime import datetime, timedelta\r\n",
        "import pandas as pd\r\n",
        "from notebookutils import mssparkutils"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2021-08-12T21:53:50.069941Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2021-08-12T21:53:52.1421167Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"header\", \"true\").json(ResultBlobPath)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2021-08-12T21:53:50.1581606Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2021-08-12T21:53:52.142885Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2021-08-12T21:53:50.2959625Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2021-08-12T21:53:52.1435092Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'Attachments', 'BccRecipients', 'BodyPreview', 'Categories', 'CcRecipients', 'ChangeKey', 'ConversationId', 'ConversationIndex', 'CreatedDateTime', 'Flag', 'From', 'HasAttachments', 'Id', 'Importance', 'InferenceClassification', 'InternetMessageId', 'IsComplete', 'IsDeliveryReceiptRequested', 'IsDraft', 'IsRead', 'IsReadReceiptRequested', 'LastModifiedDateTime', 'LikesPreview', 'Mentions', 'MentionsPreview', 'MultiValueExtendedProperties', 'ParentFolderId', 'RawUniqueBody', 'ReceivedDateTime', 'ReplyTo', 'Sender', 'SentDateTime', 'SingleValueExtendedProperties', 'Subject', 'ToRecipients', 'UniqueBody', 'UnsubscribeData', 'UnsubscribeEnabled', 'UserEmailAddress', 'WebLink', 'folderDisplayName', 'ptenant', 'puser'\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def email_extractor(s):\r\n",
        "    \"\"\"\r\n",
        "    This extracts the email addresses from the Sender and recipients fields and creates a comma-separated string\r\n",
        "    \"\"\"\r\n",
        "    string = s.split(\"u'\")\r\n",
        "    j = \"\"\r\n",
        "    string_flag = 0\r\n",
        "    for x in string:\r\n",
        "        if '@' in x:\r\n",
        "            if string_flag == 0:\r\n",
        "                j += x[0:x.find(\"'\")]\r\n",
        "                string_flag = 1\r\n",
        "            else:\r\n",
        "                j += \", \" + x[0:x.find(\"'\")]\r\n",
        "                    \r\n",
        "    return j\r\n",
        "\r\n",
        "# User defined function for extracting emails\r\n",
        "uuidUdf= udf(lambda x: email_extractor(str(x)))\r\n",
        "recipient_udf = udf(lambda x,y,z: email_extractor(str(x)+str(y)+str(z)))\r\n",
        "\r\n",
        "# Create the P2P matrix from email data using time sent, senders and recipients\r\n",
        "\r\n",
        "email2 = (df\r\n",
        "    .select(\"SentDateTime\",\"Sender\",\"ToRecipients\",\"CcRecipients\",\"BccRecipients\")\r\n",
        "    .withColumn(\"Node1\",df.Sender.EmailAddress.Address)\r\n",
        "    .withColumn(\"Node2\",recipient_udf(col(\"ToRecipients\"),col(\"CcRecipients\"),col(\"BccRecipients\")))\r\n",
        "    .select(\"SentDateTime\",\"Node1\",\"Node2\")\r\n",
        "    .withColumn(\"Node2\",explode(split(col(\"Node2\"),\",\")))\r\n",
        "    .withColumn(\"MonthEndDate\",last_day(col(\"SentDateTime\")))\r\n",
        "    .withColumn(\"MonthStartDate\",trunc(col(\"SentDateTime\"),\"month\"))\r\n",
        "    .drop(\"SentDateTime\"))\r\n",
        "    \r\n",
        "email2 = (df\r\n",
        "    .select(\"SentDateTime\",\"Sender\",\"ToRecipients\",\"CcRecipients\",\"BccRecipients\")\r\n",
        "    .withColumn(\"Node1\", df.Sender.EmailAddress.Address)\r\n",
        "    .withColumn(\"Node2\", df.ToRecipients.EmailAddress.Address)\r\n",
        "    .select(\"SentDateTime\",\"Node1\",\"Node2\")\r\n",
        "    .withColumn(\"Node2\",explode(col(\"Node2\")))\r\n",
        "    .withColumn(\"MonthEndDate\",last_day(col(\"SentDateTime\")))\r\n",
        "    .withColumn(\"MonthStartDate\",trunc(col(\"SentDateTime\"),\"month\"))\r\n",
        "    .drop(\"SentDateTime\"))\r\n",
        "    \r\n",
        "# Create a mapping of email addresses to unique integers/identifiers for quicker processing\r\n",
        "participants_map = (email2.select(\"Node1\").distinct()\r\n",
        "    .union(email2.select(\"Node2\").distinct())\r\n",
        "    .distinct()\r\n",
        "    .withColumn(\"PID\",monotonically_increasing_id()+1)).cache()\r\n",
        "\r\n",
        "# Create the Node1Pid, Node2Pid, Node1Phid, Node2Phid, WeightbyHours and WeightbyCount columns\r\n",
        "email2 = (email2\r\n",
        "          .groupby(\"MonthStartDate\",\"MonthEndDate\",\"Node1\",\"Node2\").count()\r\n",
        "          .withColumnRenamed(\"count\",\"WeightbyCount\")\r\n",
        "          .withColumn(\"WeightbyHours\", col(\"WeightbyCount\")*5/60)\r\n",
        "          .join(participants_map,on=\"Node1\",how=\"left\").withColumnRenamed(\"PID\",\"Node1Pid\")\r\n",
        "          .join(participants_map.withColumnRenamed(\"Node1\",\"Node2\"), on=\"Node2\", how=\"left\").withColumnRenamed(\"PID\",\"Node2Pid\")\r\n",
        "          .withColumn(\"Node1Phid\", col(\"Node1Pid\"))\r\n",
        "          .withColumn(\"Node2Phid\", col(\"Node2Pid\"))\r\n",
        "         .drop(\"Node1\",\"Node2\")\r\n",
        "         .select('Node1Pid','Node2Pid','Node1Phid','Node2Phid', 'WeightbyHours', 'WeightbyCount','MonthStartDate', 'MonthEndDate')).cache()\r\n",
        "\r\n",
        "# Put into a sql table for data exploration\r\n",
        "display(email2)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2021-08-12T21:53:50.8869589Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2021-08-12T21:53:52.1443245Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ResultBlobPath = 'abfss://users@dopsis.dfs.core.windows.net/ona/Org_Insights/'\r\n",
        "def df_output_blob(df, extension, outFolder):\r\n",
        "    outPath = ResultBlobPath + outFolder + \"/\"\r\n",
        "    if extension == 'csv':\r\n",
        "      df.repartition(1).write.csv(outPath, header='true', mode='overwrite', escape=\"\\\"\")\r\n",
        "    elif extension == 'json':\r\n",
        "      df.repartition(1).write.json(outPath)\r\n",
        "    \r\n",
        "    # Copy file from outFolder to central working directory\r\n",
        "    try:\r\n",
        "      fullLS = mssparkutils.fs.ls(outPath)\r\n",
        "      for i in fullLS:\r\n",
        "        if 'part-00000' in i.name:\r\n",
        "          outFileName = i.name\r\n",
        "          outFileLocation = i.path\r\n",
        "          newFileLocation = ResultBlobPath + outFolder + '.' + extension\r\n",
        "          mssparkutils.fs.mv(outFileLocation, newFileLocation, True)\r\n",
        "          print ('File moved successfully: ', newFileLocation)\r\n",
        "    except Exception as e:\r\n",
        "      print (\"Error moving file. Error: \", e) \r\n",
        "  \r\n",
        "    # clean up old files\r\n",
        "    try:\r\n",
        "      mssparkutils.fs.rm(outPath , True)\r\n",
        "      print ('Work Folder deleted: ', outPath)\r\n",
        "    except Exception as e:\r\n",
        "      print (\"Error Deleting work File or Folder. Error: \", e)\r\n",
        "      \r\n",
        "email2.name = \"MGDC_contoso_P2P\"\r\n",
        "df_output_blob(email2,\"csv\",\"MGDC\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2021-08-12T21:53:51.0163178Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2021-08-12T21:53:52.1449967Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(email2.select(\"MonthStartDate\",\"MonthEndDate\").groupBy(\"MonthStartDate\").min())"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2021-08-12T21:53:51.0937369Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2021-08-12T21:53:52.1458335Z"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}