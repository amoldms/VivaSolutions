{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Assumptions <br>\r\n",
        "1. puser will be used as the PersonHistoricalId <br>\r\n",
        "2. imAddresses as the email address <br>\r\n",
        "3. PopulationType column assumes all are licensed employees (not true because it will include conference rooms) \r\n",
        "4. All people are assumed to be internal\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 53,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-19T20:17:27.9554916Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-19T20:17:28.0632344Z",
              "execution_finish_time": "2021-08-19T20:17:28.3125065Z"
            },
            "text/plain": "StatementMeta(spark1, 53, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Getting inputs for accessing the blob\r\n",
        "StorageAccount = \"dopsis\"\r\n",
        "BLOBcontainer = \"ona/MGDC_20210817\"\r\n",
        "RawDataContainer = \"users/rawdata/MGDC_data\"\r\n",
        "\r\n",
        "# Setting connection\r\n",
        "(input_container, raw_data_container) = (RawDataContainer.split(\"/\")[:2])\r\n",
        "(out_container, ona_container) = (BLOBcontainer.split(\"/\")[:2])\r\n",
        " \r\n",
        "wasbs_template = \"abfss://%s@%s.dfs.core.windows.net/%s/{0}/\" % (input_container, StorageAccount, raw_data_container)\r\n",
        "\r\n",
        "wasbs_template_out = \"abfss://%s@%s.blob.core.windows.net/%s/{0}/\" % (out_container, StorageAccount, ona_container)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 53,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-19T20:17:28.0342244Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-19T20:17:28.4126864Z",
              "execution_finish_time": "2021-08-19T20:17:28.5731675Z"
            },
            "text/plain": "StatementMeta(spark1, 53, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import to_date, date_format, ceil, year, lit, udf, explode, split, last_day, trunc, monotonically_increasing_id, lower\r\n",
        "import os\r\n",
        "import pyspark.sql.functions as F\r\n",
        "import datetime as dt\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import col\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "import networkx as nx\r\n",
        "import topologic as tc\r\n",
        "import graspologic as gc\r\n",
        "\r\n",
        "from datetime import datetime, timedelta\r\n",
        "import pandas as pd\r\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 53,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-19T20:17:28.0918856Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-19T20:17:28.6953104Z",
              "execution_finish_time": "2021-08-19T20:17:28.9817568Z"
            },
            "text/plain": "StatementMeta(spark1, 53, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Data Container:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "def df_output_blob(df, extension, outFolder):\r\n",
        "    outPath = ResultBlobPath + outFolder + \"/\"\r\n",
        "    if extension == 'csv':\r\n",
        "      df.repartition(1).write.csv(outPath, header='true', mode='overwrite', escape=\"\\\"\")\r\n",
        "    elif extension == 'json':\r\n",
        "      df.repartition(1).write.json(outPath)\r\n",
        "    \r\n",
        "    # Copy file from outFolder to central working directory\r\n",
        "    try:\r\n",
        "      fullLS = mssparkutils.fs.ls(outPath)\r\n",
        "      for i in fullLS:\r\n",
        "        if 'part-00000' in i.name:\r\n",
        "          outFileName = i.name\r\n",
        "          outFileLocation = i.path\r\n",
        "          newFileLocation = ResultBlobPath + outFolder + '.' + extension\r\n",
        "          mssparkutils.fs.mv(outFileLocation, newFileLocation, overwrite=True)\r\n",
        "          print ('File moved successfully: ', newFileLocation)\r\n",
        "    except Exception as e:\r\n",
        "      print (\"Error moving file. Error: \", e) \r\n",
        "  \r\n",
        "    # clean up old files\r\n",
        "    try:\r\n",
        "      mssparkutils.fs.rm(outPath , True)\r\n",
        "      print ('Work Folder deleted: ', outPath)\r\n",
        "    except Exception as e:\r\n",
        "      print (\"Error Deleting work File or Folder. Error: \", e)\r\n",
        "\r\n",
        "ResultBlobPath = wasbs_template.format('MGDC_Viva_raw')\r\n",
        "print(\"Raw Data Container: \", ResultBlobPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 53,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-19T20:17:28.1962392Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-19T20:17:29.1429131Z",
              "execution_finish_time": "2021-08-19T20:17:32.2004912Z"
            },
            "text/plain": "StatementMeta(spark1, 53, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of User Records:  186\nFile moved successfully:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/PersonHistorical.csv\nWork Folder deleted:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/PersonHistorical/"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Create the personhistorical file\r\n",
        "users = spark.read.option('header', 'true').json(wasbs_template.format('MGDC_users'))\r\n",
        "user_select_columns = [col('id').alias('PersonHistoricalId'),\r\n",
        "                        'imAddresses',\r\n",
        "                        col('createdDateTime').alias('EffectiveDate'),\r\n",
        "                        col('department').alias('Organization'),\r\n",
        "                        'jobTitle', \r\n",
        "                        col('hireDate').alias('StartDate'), \r\n",
        "                        'companyName', \r\n",
        "                        'ptenant']\r\n",
        "csvPersonHistorical = users.select(user_select_columns).withColumn('EmailAddress', F.explode('imAddresses'))\r\n",
        "csvPersonHistorical = (csvPersonHistorical  \r\n",
        "    .withColumn(\"Domain\", F.substring_index(csvPersonHistorical.EmailAddress, '@', -1))\r\n",
        "    .withColumn('PopulationType',lit('MeasuredEmployee'))\r\n",
        "    .withColumn('EndDate',lit('9999-12-31T23:59:59.9999999'))\r\n",
        "    .withColumn('IsInternal',lit(True))\r\n",
        "    .select('PersonHistoricalId','EmailAddress','StartDate','EndDate','PopulationType',\r\n",
        "    'Domain','EffectiveDate','jobTitle','companyName','ptenant','Organization','IsInternal'))\r\n",
        "\r\n",
        "print(\"Total Number of User Records: \", csvPersonHistorical.count())\r\n",
        "\r\n",
        "df_output_blob(csvPersonHistorical,'csv','PersonHistorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 53,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-19T20:18:32.0125128Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-19T20:18:32.1560136Z",
              "execution_finish_time": "2021-08-19T20:21:02.1102435Z"
            },
            "text/plain": "StatementMeta(spark1, 53, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mail count:  1073335\nMail Participants count:  2978015\nFile moved successfully:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/Mails.csv\nWork Folder deleted:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/Mails/\nFile moved successfully:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/MailParticipants.csv\nWork Folder deleted:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/MailParticipants/"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Create the Mails and MailsParticipants files\r\n",
        "# Mails.csv file\r\n",
        "### TO DO ###  Add the Cc and Bcc Recipients to the recipients list\r\n",
        "rawMails = spark.read.option(\"header\", \"true\").json(wasbs_template.format('MGDC_emails'))\r\n",
        "\r\n",
        "Mails = (rawMails\r\n",
        "    .select(col(\"Id\").alias(\"MailId\"),\"ConversationId\",\"Subject\",col(\"SentDateTime\").alias(\"SentTime\"),\"ToRecipients.EmailAddress.Address\")\r\n",
        "    .withColumn(\"SenderTimeSpentInMinutes\",lit(5)))\r\n",
        "\r\n",
        "recipients = (Mails\r\n",
        "    .select(\"MailId\",\"SentTime\",\"Address\")\r\n",
        "    .withColumn(\"Recipients\", explode(col(\"Address\")))\r\n",
        "    .groupBy(\"MailId\",\"SentTime\").agg(F.count(\"Recipients\").alias(\"NumberofRecipients\")))\r\n",
        "\r\n",
        "csvMails = (Mails.select(\"MailId\",\"ConversationId\",\"Subject\",\"SenderTimeSpentInMinutes\",\"SentTime\")\r\n",
        "    .join(recipients, on=['MailId','SentTime'], how='left')\r\n",
        "    .select(\"MailId\",\"ConversationId\",\"Subject\",\"SentTime\",\"SenderTimeSpentInMinutes\",\"NumberofRecipients\"))\r\n",
        "\r\n",
        "\r\n",
        "# MailsParticipants.csv file\r\n",
        "mp_sender = (rawMails\r\n",
        "    .select(col(\"Id\").alias(\"MailId\"),\r\n",
        "        col(\"SentDateTime\").alias(\"LocalSentTime\"),\r\n",
        "        col(\"Sender.EmailAddress.Address\").alias(\"EmailAddress\"))\r\n",
        "    .withColumn(\"IsSender\",lit(True))\r\n",
        "    .withColumn(\"PersonTimeSpentInHours\",lit(5/60))\r\n",
        "    .withColumn(\"PersonTimeSpentInMinutes\", lit(5)))\r\n",
        "\r\n",
        "mp_receiver = (rawMails\r\n",
        "    .select(col(\"Id\").alias(\"MailId\"),\r\n",
        "        col(\"SentDateTime\").alias(\"LocalSentTime\"),\r\n",
        "        col(\"ToRecipients.EmailAddress.Address\").alias(\"EmailAddress\"))\r\n",
        "    .withColumn(\"EmailAddress\", explode(col(\"EmailAddress\")))\r\n",
        "    .withColumn(\"IsSender\",lit(False))\r\n",
        "    .withColumn(\"PersonTimeSpentInHours\",lit(2.5/60))\r\n",
        "    .withColumn(\"PersonTimeSpentInMinutes\", lit(2.5)))\r\n",
        "\r\n",
        "csvMailParticipants = (mp_sender.union(mp_receiver).sort([\"LocalSentTime\",\"MailId\",\"IsSender\"])\r\n",
        "    .join(csvPersonHistorical.select(\"EmailAddress\",\"PersonHistoricalId\"), on=\"EmailAddress\", how='left')\r\n",
        "    .select(\"MailId\",\"PersonHistoricalId\",\"IsSender\",\"LocalSentTime\",\"PersonTimeSpentInHours\",\"PersonTimeSpentInMinutes\")\r\n",
        "    .where(col('PersonHistoricalId').isNotNull()))\r\n",
        "\r\n",
        "print(\"Mail count: \", csvMails.count())\r\n",
        "print(\"Mail Participants count: \", csvMailParticipants.count())\r\n",
        "\r\n",
        "df_output_blob(csvMails,'csv','Mails')\r\n",
        "df_output_blob(csvMailParticipants,'csv','MailParticipants')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "spark1",
              "session_id": 53,
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-19T20:21:56.526655Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-19T20:21:56.8436512Z",
              "execution_finish_time": "2021-08-19T20:23:13.5084756Z"
            },
            "text/plain": "StatementMeta(spark1, 53, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meeting count:  221602\nMeeting Participants count:  741451\nFile moved successfully:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/Meetings.csv\nWork Folder deleted:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/Meetings/\nFile moved successfully:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/MeetingParticipants.csv\nWork Folder deleted:  abfss://users@dopsis.dfs.core.windows.net/rawdata/MGDC_Viva_raw/MeetingParticipants/"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Create the Meetings and MeetingsParticipants files\r\n",
        "# Meetings.csv file\r\n",
        "### TO DO ###  Add the Cc and Bcc Recipients to the recipients list\r\n",
        "rawMeetings = spark.read.option(\"header\", \"true\").json(wasbs_template.format('MGDC_meetings'))\r\n",
        "\r\n",
        "Meetings = (rawMeetings\r\n",
        "    .select(col(\"Id\").alias(\"MeetingId\"),\r\n",
        "            col(\"iCalUId\").alias(\"ICalUid\"),\r\n",
        "            col(\"subject\").alias(\"Subject\"),\r\n",
        "            col(\"recurrence\").alias(\"IsRecurring\"),\r\n",
        "            col(\"isCancelled\").alias(\"IsCancelled\"),\r\n",
        "            F.to_timestamp(col(\"start.dateTime\")).alias(\"LocalStartTime\"),\r\n",
        "            F.unix_timestamp(F.to_timestamp(col(\"start.dateTime\"))).alias(\"StartTime\"),\r\n",
        "            F.unix_timestamp(F.to_timestamp(col(\"end.dateTime\"))).alias(\"EndTime\"),\r\n",
        "            col(\"attendees\"))\r\n",
        "    .withColumn(\"IsRecurring\",col(\"IsRecurring\").isNotNull())\r\n",
        "    .withColumn(\"DurationMinutes\", (col(\"EndTime\") - col(\"StartTime\"))/60)\r\n",
        "    .withColumn(\"DurationHours\", col(\"DurationMinutes\")/60)\r\n",
        "    .drop(\"EndTime\"))\r\n",
        "\r\n",
        "attendees = (rawMeetings\r\n",
        "    .select(col(\"Id\").alias(\"MeetingId\"),\r\n",
        "            col(\"iCalUId\").alias(\"ICalUid\"),\r\n",
        "            F.unix_timestamp(F.to_timestamp(col(\"start.dateTime\"))).alias(\"StartTime\"),\r\n",
        "            \"attendees.emailAddress.address\")\r\n",
        "    .withColumn(\"address\", explode(\"address\"))\r\n",
        "    .groupBy(\"MeetingId\",\"ICalUid\",\"StartTime\").agg(F.count(\"address\").alias(\"TotalAttendees\")))\r\n",
        "\r\n",
        "acceptance = (rawMeetings\r\n",
        "    .select(col(\"Id\").alias(\"MeetingId\"),\r\n",
        "            col(\"iCalUId\").alias(\"ICalUid\"),\r\n",
        "            F.unix_timestamp(F.to_timestamp(col(\"start.dateTime\"))).alias(\"StartTime\"),\r\n",
        "            \"attendees.status.response\")\r\n",
        "    .withColumn(\"response\", explode(\"response\"))\r\n",
        "    .withColumn(\"response\", F.split(col(\"response\"),\"'\")[1])\r\n",
        "    .groupBy(\"MeetingId\",\"ICalUid\",\"StartTime\").pivot(\"response\").count().fillna(0)\r\n",
        "    .withColumnRenamed(\"accepted\",\"TotalAccept\")\r\n",
        "    .withColumnRenamed(\"declined\",\"TotalDecline\")\r\n",
        "    .withColumn(\"TotalNoResponse\", col(\"none\")+col(\"notResponded\"))\r\n",
        "    .drop(\"notResponded\", \"none\"))\r\n",
        "\r\n",
        "csvMeetings = (Meetings.join(attendees, on=[\"MeetingId\", \"ICalUid\", \"StartTime\"], how=\"left\")\r\n",
        "                .join(acceptance, on=[\"MeetingId\", \"ICalUid\", \"StartTime\"], how='left')\r\n",
        "                .select('MeetingId', 'ICalUid', 'Subject', 'IsRecurring', 'IsCancelled',\r\n",
        "                col('LocalStartTime').alias(\"StartTime\"), 'DurationHours','DurationMinutes', 'TotalAccept', 'TotalDecline', 'TotalNoResponse',\r\n",
        "                'TotalAttendees'))\r\n",
        "\r\n",
        "\r\n",
        "# MeetingParticipants.csv file\r\n",
        "csvMeetingParticipants = (rawMeetings\r\n",
        "    .select(col(\"Id\").alias(\"MeetingId\"),\r\n",
        "            col(\"attendees\"),\r\n",
        "            F.to_timestamp(col(\"start.dateTime\")).alias(\"UTCStartTime\"),\r\n",
        "            F.unix_timestamp(F.to_timestamp(col(\"start.dateTime\"))).alias(\"StartTime\"),\r\n",
        "            F.unix_timestamp(F.to_timestamp(col(\"end.dateTime\"))).alias(\"EndTime\"),\r\n",
        "            col('organizer.emailAddress.address').alias('Organizer'))\r\n",
        "    .withColumn(\"DurationMinutesAdjusted\", (col(\"EndTime\") - col(\"StartTime\"))/60)\r\n",
        "    .withColumn(\"new\", F.arrays_zip(\"attendees.emailAddress.address\", \"attendees.status.response\"))\r\n",
        "    .withColumn(\"new\", F.explode(\"new\"))\r\n",
        "    .withColumn(\"EmailAddress\",col(\"new.0\"))\r\n",
        "    .withColumn(\"Response\", F.split(col(\"new.1\"),\"'\")[1])\r\n",
        "    .withColumn(\"IsOrganizer\",col(\"EmailAddress\")==col(\"Organizer\"))\r\n",
        "    .join(csvPersonHistorical.select(\"EmailAddress\",\"PersonHistoricalId\"), on=\"EmailAddress\", how='left')\r\n",
        "    .select(\"MeetingId\",\"PersonHistoricalId\",\"UTCStartTime\",\"IsOrganizer\",\"Response\",\"DurationMinutesAdjusted\")\r\n",
        "    .where(col('PersonHistoricalId').isNotNull()))\r\n",
        "\r\n",
        "print(\"Meeting count: \", csvMeetings.count())\r\n",
        "print(\"Meeting Participants count: \", csvMeetingParticipants.count())\r\n",
        "\r\n",
        "df_output_blob(csvMeetings,'csv','Meetings')\r\n",
        "df_output_blob(csvMeetingParticipants,'csv','MeetingParticipants')"
      ]
    }
  ]
}